{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/utfsm.png\" alt=\"\" width=\"200px\" align=\"right\"/>\n",
    "# USM Numérica\n",
    "## Tema del Notebook\n",
    "### Objetivos\n",
    "1. Conocer el funcionamiento de la librerìa sklearn de Machine Learning\n",
    "2. Aplicar la librerìa sklearn para solucionar problemas de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sobre el autor\n",
    "### Sebastián Flores\n",
    "#### ICM UTFSM\n",
    "#### sebastian.flores@usm.cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sobre la presentación\n",
    "#### Contenido creada en ipython notebook (jupyter)\n",
    "#### Versión en Slides gracias a RISE de Damián Avila\n",
    "Software:\n",
    "* python 2.7 o python 3.1\n",
    "* pandas 0.16.1\n",
    "* sklearn 0.16.1\n",
    "\n",
    "Opcional:\n",
    "* numpy 1.9.2\n",
    "* matplotlib 1.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import __version__ as vsn\n",
    "print(vsn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 0.1 Instrucciones\n",
    "Las instrucciones de instalación y uso de un ipython notebook se encuentran en el siguiente [link](link).\n",
    "\n",
    "Después de descargar y abrir el presente notebook, recuerden:\n",
    "* Desarrollar los problemas de manera secuencial.\n",
    "* Guardar constantemente con *`Ctr-S`* para evitar sorpresas.\n",
    "* Reemplazar en las celdas de código donde diga *`FIX_ME`* por el código correspondiente.\n",
    "* Ejecutar cada celda de código utilizando *`Ctr-Enter`*\n",
    "\n",
    "## 0.2 Licenciamiento y Configuración\n",
    "Ejecutar la siguiente celda mediante *`Ctr-Enter`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IPython Notebook v4.0 para python 3.0\n",
    "Librerías adicionales: numpy, scipy, matplotlib. (EDITAR EN FUNCION DEL NOTEBOOK!!!)\n",
    "Contenido bajo licencia CC-BY 4.0. Código bajo licencia MIT. \n",
    "(c) Sebastian Flores, Christopher Cooper, Alberto Rubio, Pablo Bunout.\n",
    "\"\"\"\n",
    "# Configuración para recargar módulos y librerías dinámicamente\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Configuración para graficos en línea\n",
    "%matplotlib inline\n",
    "\n",
    "# Configuración de estilo\n",
    "from IPython.core.display import HTML\n",
    "HTML(open(\"./style/slides.css\", \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.- Sobre la librería sklearn\n",
    "#### Historia\n",
    "- Nace en 2007, como un Google Summer Project de David Cournapeau. \n",
    "- Retomado por Matthieu Brucher para su proyecto de tesis.\n",
    "- Desde 2010 con soporte por parte de INRIA.\n",
    "- Actualmente +35 colaboradores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.- Sobre la librería sklearn\n",
    "#### Instalación\n",
    "En python, con un poco de suerte:\n",
    "```\n",
    "pip install -U scikit-learn\n",
    "```\n",
    "\n",
    "Utilizando Anaconda:\n",
    "\n",
    "```\n",
    "conda install scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.- Sobre la librería sklearn\n",
    "#### ¿Porqué sklearn?\n",
    "sklearn viene de scientific toolbox for Machine Learning. \n",
    "\n",
    "scikit learn para los amigos.\n",
    "\n",
    "Existen múltiples scikits, que son \"scientific toolboxes\" construidos sobre SciPy: [https://scikits.appspot.com/scikits](https://scikits.appspot.com/scikits)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Primero que nada... ¿Qué es Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.- Machine Learning 101\n",
    "#### Ejemplo\n",
    "\n",
    "Consideremos un dataset consistente en características de diversos animales.\n",
    "\n",
    "```\n",
    "patas,   ancho,   largo,   alto,    peso,        especie\n",
    "[numero],[metros],[metros],[metros],[kilogramos],[]\n",
    "2,       0.6,     0.4,     1.7,     75,          humano\n",
    "2,       0.6,     0.4,     1.8,     90,          humano\n",
    "...\n",
    "2,       0.5,     0.5,     1.7,     85,          humano\n",
    "4,       0.2,     0.5,     0,3,     30,          gato\n",
    "...\n",
    "4,       0.25,    0.55,    0.32,    32,          gato\n",
    "4,       0.5,     0.8,     0.3,     50,          perro\n",
    "...\n",
    "4,       0.4,     0.4,     0.32,    40,          perro\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.- Machine Learning 101\n",
    "### Clustering\n",
    "\n",
    "Supongamos que no nos han dicho la especie de cada animal. \n",
    "\n",
    "¿Podríamos reconocer las distintas especies? \n",
    "\n",
    "¿Podríamos reconocer que existen 3 grupos distintos de animales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.- Machine Learning 101\n",
    "### Clasificación\n",
    "\n",
    "Supongamos que conocemos los datos de cada animal y además la especie.\n",
    "\n",
    "Si alguien llega con las medidas de un animal... ¿podemos decir cuál será la especie?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.- Machine Learning 101\n",
    "### Regresión\n",
    "\n",
    "Supongamos que conocemos los datos de cada animal y su especie. \n",
    "\n",
    "Si alguien llega con los datos de un animal, excepto el peso... ¿podemos predecir el peso que tendrá el animal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.- Machine Learning 101\n",
    "### Definiciones\n",
    "\n",
    "* Los datos utilizados para predecir son predictores (features), y típicamente se llama `X`.\n",
    "* El dato que se busca predecir se llama etiqueta (label) y puede ser numérica o categórica, y típicamente se llama `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3- Generalidades de sklearn\n",
    "### Imagen resumen\n",
    "\n",
    "<img src=\"images/ml_map.png\" alt=\"\" width=\"1400px\" align=\"middle\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3- Generalidades de sklearn\n",
    "### Procedimiento General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import HelpfulMethods\n",
    "from sklearn import AlgorithmIWantToUse\n",
    "\n",
    "# split data into train and test datasets\n",
    "\n",
    "# train model with train dataset\n",
    "\n",
    "# compute error on test dataset\n",
    "\n",
    "# Optional: Train model with all available data\n",
    "\n",
    "# Use model for some  prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4- Clustering con sklearn\n",
    "#### Wine Dataset\n",
    "\n",
    "Los datos del [Wine Dataset](https://archive.ics.uci.edu/ml/datasets/Wine) son un conjunto de datos clásicos para verificar los algoritmos de clustering. \n",
    "\n",
    "<img src=\"images/wine.jpg\" alt=\"\" width=\"600px\" align=\"middle\"/>\n",
    "\n",
    "Los datos corresponden a 3 cultivos diferentes de vinos de la misma región de Italia, y que han sido identificados con las etiquetas 1, 2 y 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4- Clustering con sklearn\n",
    "#### Wine Dataset\n",
    "\n",
    "Para cada tipo de vino se realizado 13 análisis químicos:\n",
    "\n",
    "1. Alcohol \n",
    "2.  Malic acid \n",
    "3. Ash \n",
    "4. Alcalinity of ash \n",
    "5. Magnesium \n",
    "6. Total phenols \n",
    "7. Flavanoids \n",
    "8. Nonflavanoid phenols \n",
    "9. Proanthocyanins \n",
    "10. Color intensity \n",
    "11. Hue \n",
    "12. OD280/OD315 of diluted wines \n",
    "13. Proline \n",
    "\n",
    "\n",
    "La base de datos contiene 178 muestras distintas en total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "head data/wine_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4- Clustering con sklearn\n",
    "#### Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"data/wine_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4- Clustering con sklearn\n",
    "#### Exploración de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4- Clustering con sklearn\n",
    "#### Exploración gráfica de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "data.hist(figsize=(12,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#pd.scatter_matrix(data, figsize=(12,12), range_padding=0.2)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4- Clustering con sklearn\n",
    "#### Separación de los datos\n",
    "Necesitamos separar los datos en los predictores (features) y las etiquetas (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = data.drop(\"class\", axis=1)\n",
    "true_labels = data[\"class\"] -1 # labels deben ser 0, 1, 2, ..., n-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4- Custering\n",
    "#### Magnitudes de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4- Clustering con sklearn\n",
    "#### Algoritmo de Clustering\n",
    "Para Clustering usaremos el algoritmo KMeans. \n",
    "\n",
    "Apliquemos un algoritmo de clustering directamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Parameters\n",
    "n_clusters = 3\n",
    "\n",
    "# Running the algorithm\n",
    "kmeans = KMeans(n_clusters)\n",
    "kmeans.fit(X)\n",
    "pred_labels = kmeans.labels_\n",
    "\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4- Clustering con sklearn\n",
    "#### Normalizacion de datos\n",
    "Resulta conveniente escalar los datos, para que el algoritmo de clustering funcione mejor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "X_scaled = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_scaled.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_scaled.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4- Clustering con sklearn\n",
    "#### Algoritmo de Clustering\n",
    "Ahora podemos aplicar un algoritmo de clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Parameters\n",
    "n_clusters = 3\n",
    "\n",
    "# Running the algorithm\n",
    "kmeans = KMeans(n_clusters)\n",
    "kmeans.fit(X_scaled)\n",
    "pred_labels = kmeans.labels_\n",
    "\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4- Clustering con sklearn\n",
    "#### Regla del codo\n",
    "En todos los casos hemos utilizado que el número de clusters es igual a 3. En caso que no conociéramos este dato, deberíamos graficar la suma de las distancias a los clusters para cada punto, en función del número de clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clusters = range(2,20)\n",
    "total_distance = []\n",
    "for n_clusters in clusters:\n",
    "    kmeans = KMeans(n_clusters)\n",
    "    kmeans.fit(X_scaled)\n",
    "    pred_labels = kmeans.labels_\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    # Get the distances\n",
    "    distance_for_n = 0\n",
    "    for k in range(n_clusters):\n",
    "        points = X_scaled[pred_labels==k]\n",
    "        aux = (points - centroids[k,:])**2\n",
    "        distance_for_n += (aux.sum(axis=1)**0.5).sum()\n",
    "    total_distance.append(distance_for_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4- Clustering con sklearn\n",
    "Graficando lo anterior, obtenemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "plt.plot(clusters, total_distance, 'rs')\n",
    "plt.xlim(min(clusters)-1, max(clusters)+1)\n",
    "plt.ylim(0, max(total_distance)*1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4- Clustering con sklearn\n",
    "¿Qué tan dificil es usar otro algoritmo de clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nada dificil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Algoritmos disponibles:\n",
    "* K-Means\n",
    "* Mini-batch K-means\n",
    "* Affinity propagation\n",
    "* Mean-shift\n",
    "* Spectral clustering\n",
    "* Ward hierarchical clustering\n",
    "* Agglomerative clustering\n",
    "* DBSCAN\n",
    "* Gaussian mixtures\n",
    "* Birch\n",
    "\n",
    "Lista con detalles: [http://scikit-learn.org/stable/modules/clustering.html](http://scikit-learn.org/stable/modules/clustering.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Normalization of data\n",
    "X_scaled = preprocessing.scale(X)\n",
    "\n",
    "# Running the algorithm\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X_scaled)\n",
    "pred_labels = kmeans.labels_\n",
    "\n",
    "# Evaluating the output\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Normalization of data\n",
    "X_scaled = preprocessing.scale(X)\n",
    "\n",
    "# Running the algorithm\n",
    "kmeans = MiniBatchKMeans(n_clusters=3)\n",
    "kmeans.fit(X_scaled)\n",
    "pred_labels = kmeans.labels_\n",
    "\n",
    "# Evaluating the output\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Normalization of data\n",
    "X_scaled = preprocessing.scale(X)\n",
    "\n",
    "# Running the algorithm\n",
    "kmeans = AffinityPropagation(preference=-300)\n",
    "kmeans.fit(X_scaled)\n",
    "pred_labels = kmeans.labels_\n",
    "\n",
    "# Evaluating the output\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5- Clasificación\n",
    "#### Reconocimiento de dígitos\n",
    "Los datos se encuentran en 2 archivos, `data/optdigits.train` y `data/optdigits.test`. \n",
    "\n",
    "Como su nombre lo indica, el set `data/optdigits.train` contiene los ejemplos que deben ser usados para entrenar el modelo, mientras que el set `data/optdigits.test` se utilizará para obtener una estimación del error de predicción.\n",
    "\n",
    "Ambos archivos comparten el mismo formato: cada línea contiene 65 valores. Los 64 primeros corresponden a la representación de la imagen en escala de grises (0-blanco, 255-negro), y el valor 65 corresponde al dígito de la imágen (0-9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5- Clasificación\n",
    "#### Cargando los datos\n",
    "\n",
    "Para cargar los datos, utilizamos np.loadtxt con los parámetros extra delimiter (para indicar que el separador será en esta ocasión una coma) y con el dype np.int8 (para que su representación en memoria sea la mínima posible, 8 bits en vez de 32/64 bits para un float)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "XY_tv = np.loadtxt(\"data/optdigits.train\", delimiter=\",\", dtype=np.int8)\n",
    "print(XY_tv)\n",
    "X_tv = XY_tv[:,:64]\n",
    "Y_tv = XY_tv[:, 64]\n",
    "\n",
    "print(X_tv.shape)\n",
    "print(Y_tv.shape)\n",
    "print(X_tv[0,:])\n",
    "print(X_tv[0,:].reshape(8,8))\n",
    "print(Y_tv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5- Clasificación\n",
    "#### Visualizando los datos\n",
    "\n",
    "Para visualizar los datos utilizaremos el método imshow de pyplot. Resulta necesario convertir el arreglo desde las dimensiones (1,64)  a (8,8) para que la imagen sea cuadrada y pueda distinguirse el dígito. Superpondremos además el label correspondiente al dígito, mediante el método text. Realizaremos lo anterior para los primeros 25 datos del archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Well plot the first nx*ny examples\n",
    "nx, ny = 5, 5\n",
    "fig, ax = plt.subplots(nx, ny, figsize=(12,12))\n",
    "for i in range(nx):\n",
    "    for j in range(ny):\n",
    "        index = j+ny*i\n",
    "        data  = X_tv[index,:].reshape(8,8)\n",
    "        label = Y_tv[index]\n",
    "        ax[i][j].imshow(data, interpolation='nearest', cmap=plt.get_cmap('gray_r'))\n",
    "        ax[i][j].text(7, 0, str(int(label)), horizontalalignment='center',\n",
    "                verticalalignment='center', fontsize=10, color='blue')\n",
    "        ax[i][j].get_xaxis().set_visible(False)\n",
    "        ax[i][j].get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5- Clasificación\n",
    "#### Entrenamiento trivial\n",
    "Para clasificar utilizaremos el algoritmo K Nearest Neighbours.\n",
    "\n",
    "Entrenaremos el modelo con 1 vecino y verificaremos el error de predicción en el set de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k = 1\n",
    "kNN = KNeighborsClassifier(n_neighbors=k)\n",
    "kNN.fit(X_tv, Y_tv)\n",
    "Y_pred = kNN.predict(X_tv)\n",
    "n_errors = sum(Y_pred!=Y_tv)\n",
    "print(\"Hay %d errores de un total de %d ejemplos de entrenamiento\" %(n_errors, len(Y_tv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "¡La mejor predicción del punto es el mismo punto! \n",
    "\n",
    "Pero esto generalizaría catastróficamente.\n",
    "\n",
    "Es importantísimo **entrenar** en un set de datos y luego probar como generaliza/funciona en un set **completamente nuevo**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5- Clasificación\n",
    "#### Seleccionando el número adecuado de vecinos\n",
    "\n",
    "Buscando el valor de k más apropiado\n",
    "\n",
    "A partir del análisis del punto anterior, nos damos cuenta de la necesidad de:\n",
    "1. Calcular el error en un set distinto al utilizado para entrenar.\n",
    "2. Calcular el mejor valor de vecinos para el algoritmo.\n",
    "\n",
    "(Esto tomará un tiempo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "template = \"k={0:,d}: {1:.1f} +- {2:.1f} errores de clasificación de un total de {3:,d} puntos\"\n",
    "# Fitting the model\n",
    "mean_error_for_k = []\n",
    "std_error_for_k = []\n",
    "k_range = range(1,8)\n",
    "for k in k_range:\n",
    "    errors_k = []\n",
    "    for i in range(10):\n",
    "        kNN = KNeighborsClassifier(n_neighbors=k)\n",
    "        X_train, X_valid, Y_train, Y_valid = train_test_split(X_tv, Y_tv, train_size=0.75)\n",
    "        kNN.fit(X_train, Y_train)\n",
    "        # Predicting values\n",
    "        Y_valid_pred = kNN.predict(X_valid)\n",
    "        # Count the errors\n",
    "        n_errors = sum(Y_valid!=Y_valid_pred)\n",
    "        # Add them to vector\n",
    "        errors_k.append(100.*n_errors/len(Y_valid))\n",
    "\n",
    "    errors = np.array(errors_k)\n",
    "    print(template.format(k, errors.mean(), errors.std(), len(Y_valid)))\n",
    "    mean_error_for_k.append(errors.mean())\n",
    "    std_error_for_k.append(errors.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5- Clasificación\n",
    "Podemos visualizar los datos anteriores utilizando el siguiente código, que requiere que `sd_error_for k` y `mean_error_for_k` hayan sido apropiadamente definidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = np.array(mean_error_for_k)\n",
    "std = np.array(std_error_for_k)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(k_range, mean - std, \"k:\")\n",
    "plt.plot(k_range, mean , \"r.-\")\n",
    "plt.plot(k_range, mean + std, \"k:\")\n",
    "plt.xlabel(\"Numero de vecinos k\")\n",
    "plt.ylabel(\"Error de clasificacion\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5- Clasificación\n",
    "#### Entrenando todo el modelo\n",
    "A partir de lo anterior, se fija el número de vecinos $k=3$ y se procede a entrenar el modelo con todos los datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "k = 3\n",
    "kNN = KNeighborsClassifier(n_neighbors=k)\n",
    "kNN.fit(X_tv, Y_tv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5- Clasificación\n",
    "#### Predicción en testing dataset\n",
    "\n",
    "Ahora que el modelo kNN ha sido completamente entrenado, calcularemos el error de predicción en un set de datos completamente nuevo: el set de testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cargando el archivo data/optdigits.tes\n",
    "XY_test = np.loadtxt(\"data/optdigits.test\", delimiter=\",\")\n",
    "X_test = XY_test[:,:64]\n",
    "Y_test = XY_test[:, 64]\n",
    "# Predicción de etiquetas\n",
    "Y_pred = kNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5- Clasificación\n",
    "Puesto que tenemos las etiquetas verdaderas en el set de entrenamiento, podemos visualizar que números han sido correctamente etiquetados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Mostrar los datos correctos\n",
    "mask = (Y_pred==Y_test)\n",
    "X_aux = X_test[mask]\n",
    "Y_aux_true = Y_test[mask]\n",
    "Y_aux_pred = Y_pred[mask]\n",
    "\n",
    "# We'll plot the first 100 examples, randomly choosen\n",
    "nx, ny = 5, 5\n",
    "fig, ax = plt.subplots(nx, ny, figsize=(12,12))\n",
    "for i in range(nx):\n",
    "    for j in range(ny):\n",
    "        index = j+ny*i\n",
    "        data  = X_aux[index,:].reshape(8,8)\n",
    "        label_pred = str(int(Y_aux_pred[index]))\n",
    "        label_true = str(int(Y_aux_true[index]))\n",
    "        ax[i][j].imshow(data, interpolation='nearest', cmap=plt.get_cmap('gray_r'))\n",
    "        ax[i][j].text(0, 0, label_pred, horizontalalignment='center',\n",
    "                verticalalignment='center', fontsize=10, color='green')\n",
    "        ax[i][j].text(7, 0, label_true, horizontalalignment='center',\n",
    "                verticalalignment='center', fontsize=10, color='blue')\n",
    "        ax[i][j].get_xaxis().set_visible(False)\n",
    "        ax[i][j].get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5- Clasificación\n",
    "#### Visualización de etiquetas incorrectas\n",
    "Más interesante que el gráfico anterior, resulta considerar los casos donde los dígitos han sido incorrectamente etiquetados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Mostrar los datos correctos\n",
    "mask = (Y_pred!=Y_test)\n",
    "X_aux = X_test[mask]\n",
    "Y_aux_true = Y_test[mask]\n",
    "Y_aux_pred = Y_pred[mask]\n",
    "\n",
    "# We'll plot the first 100 examples, randomly choosen\n",
    "nx, ny = 5, 5\n",
    "fig, ax = plt.subplots(nx, ny, figsize=(12,12))\n",
    "for i in range(nx):\n",
    "    for j in range(ny):\n",
    "        index = j+ny*i\n",
    "        data  = X_aux[index,:].reshape(8,8)\n",
    "        label_pred = str(int(Y_aux_pred[index]))\n",
    "        label_true = str(int(Y_aux_true[index]))\n",
    "        ax[i][j].imshow(data, interpolation='nearest', cmap=plt.get_cmap('gray_r'))\n",
    "        ax[i][j].text(0, 0, label_pred, horizontalalignment='center',\n",
    "                verticalalignment='center', fontsize=10, color='red')\n",
    "        ax[i][j].text(7, 0, label_true, horizontalalignment='center',\n",
    "                verticalalignment='center', fontsize=10, color='blue')\n",
    "        ax[i][j].get_xaxis().set_visible(False)\n",
    "        ax[i][j].get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5- Clasificación\n",
    "#### Análisis del error\n",
    "\n",
    "Después de la exploración visual de los resultados, queremos obtener el error de predicción real del modelo.\n",
    "\n",
    "¿Existen dígitos más fáciles o difíciles de clasificar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Error global\n",
    "mask = (Y_pred!=Y_test)\n",
    "error_prediccion = 100.*sum(mask) / len(mask)\n",
    "print(\"Error de predicción total de {0:.1f} %\".format(error_prediccion))\n",
    "\n",
    "for digito in range(0,10):\n",
    "    mask_digito = Y_test==digito\n",
    "    Y_test_digito = Y_test[mask_digito] \n",
    "    Y_pred_digito = Y_pred[mask_digito]\n",
    "    mask = Y_test_digito!=Y_pred_digito\n",
    "    error_prediccion = 100.*sum((Y_pred_digito!=Y_test_digito)) / len(Y_pred_digito)\n",
    "    print(\"Error de predicción para digito {0:d} de {1:.1f} %\".format(digito, error_prediccion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5- Clasificación\n",
    "#### Análisis del error (cont. de)\n",
    "\n",
    "El siguiente código muestra el error de clasificación, permitiendo verificar que números son confundibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix as cm\n",
    "cm = cm(Y_test, Y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# As in http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.jet):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(10)\n",
    "    plt.xticks(tick_marks, tick_marks)\n",
    "    plt.yticks(tick_marks, tick_marks)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "# Compute confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize the confusion matrix by row (i.e by the number of samples in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5- Clasificación\n",
    "A partir de lo anterior, vemos observamos que los mayores errores son:\n",
    "* El 2 puede clasificarse erróneamente como 1 (pero no viceversa).\n",
    "* El 7 puede clasificarse erróneamente como 9 (pero no viceversa).\n",
    "* El 8 puede clasificarse erróneamente como 1 (pero no viceversa).\n",
    "* El 9 puede clasificarse erróneamente como 3 (pero no viceversa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5- Clasificación\n",
    "#### Preguntas\n",
    "¿Es éste el mejor método de clasificación? ¿Qué otros métodos pueden utilizarse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Múltiples familias de algoritmos:\n",
    "* Logistic Regression\n",
    "* Naive Bayes\n",
    "* Decision Trees\n",
    "* Random Forests\n",
    "* Support Vector Machines\n",
    "* Neural Networks\n",
    "* Etc etc\n",
    "\n",
    "link: [http://scikit-learn.org/stable/supervised_learning.html](http://scikit-learn.org/stable/supervised_learning.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5- Conclusión\n",
    "\n",
    "\n",
    "Sklearn tiene muchos algoritmos implementados y es fácil de usar.\n",
    "\n",
    "Sin embargo, hay qu tener presente GIGO: Garbage In, Garbage Out:\n",
    "* Exploración y visualización inicial de datos.\n",
    "* Limpieza de datos\n",
    "* Utilización del algoritmo requiere conocer su funconamiento para mejor tuneo de parámetros.\n",
    "* Es bueno y fácil probar más de un algoritmo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5- Conclusión\n",
    "\n",
    "\n",
    "Y por último:\n",
    "\n",
    "* Aplicación de algoritmos de ML es delicado porque requiere (1) conocer bien los datos y (2) entender las limitaciones del algoritmo. \n",
    "* Considerar siempre una muestra para entrenamiento y una muestra para testeo: predicción es inútil si no se entrega un margen de error para la predicción.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5- Conclusión\n",
    "\n",
    "\n",
    "# ¡Gracias!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
