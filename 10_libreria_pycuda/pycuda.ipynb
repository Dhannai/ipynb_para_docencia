{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/utfsm.png\" alt=\"\" width=\"200px\" align=\"right\"/>\n",
    "# USM Numérica\n",
    "## Librerías de Python: PyCUDA\n",
    "### Objetivos\n",
    "1. Entender la utilidad de aceleradores (como las tarjetas gráficas).\n",
    "2. Aprender los fundamentos de la programación en tarjetas gráficas.\n",
    "2. Conocer la librería PyCUDA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 0.1 Instrucciones\n",
    "Las instrucciones de instalación y uso de un ipython notebook se encuentran en el siguiente [link](link).\n",
    "\n",
    "Después de descargar y abrir el presente notebook, recuerden:\n",
    "* Desarrollar los problemas de manera secuencial.\n",
    "* Guardar constantemente con *`Ctr-S`* para evitar sorpresas.\n",
    "* Reemplazar en las celdas de código donde diga *`FIX_ME`* por el código correspondiente.\n",
    "* Ejecutar cada celda de código utilizando *`Ctr-Enter`*\n",
    "\n",
    "## 0.2 Licenciamiento y Configuración\n",
    "Ejecutar la siguiente celda mediante *`Ctr-Enter`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "/*********************************************\n",
       " * CHANGE CURSIVE FOR RED\n",
       " *********************************************/\n",
       "em {font-style: normal !important;\n",
       "    color: #800000;}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "IPython Notebook v4.0 para python 3.0\n",
    "Librerías adicionales: numpy, scipy, matplotlib. (EDITAR EN FUNCION DEL NOTEBOOK!!!)\n",
    "Contenido bajo licencia CC-BY 4.0. Código bajo licencia MIT. \n",
    "(c) Sebastian Flores, Christopher Cooper, Alberto Rubio, Pablo Bunout.\n",
    "\"\"\"\n",
    "# Configuración para recargar módulos y librerías dinámicamente\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Configuración para graficos en línea\n",
    "%matplotlib inline\n",
    "\n",
    "# Configuración de estilo\n",
    "from IPython.core.display import HTML\n",
    "HTML(open(\"./style/style.css\", \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido\n",
    "1. Computación de alto desempeño (HPC)\n",
    "    * Aceleradores\n",
    "    * El rol de Python en HPC\n",
    "2. Programación en tarjetas gráficas\n",
    "    * La tarjeta gráfica\n",
    "    * Lenguaje CUDA\n",
    "3. PyCUDA\n",
    "    * Usando la tarjeta gráfica desde Python\n",
    "    * Creando tus propios \"kernels\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Computación de alto desempeño\n",
    "\n",
    "En muchos problemas de ingeniería (en verdad, en la gran mayoría de ellos) es necesario hacer una gran catidad de cálculos para lograr el objetivo. Por ejemplo, la gente que modela dinámica de fluidos con el computador, necesita resolver una ecuación en un dominio con varios millones de nodos. Otro caso es el procesamiento de datos de observatorios astronómicos, que pueden generar varios terabytes de datos por día. Los computadores convencionales se quedan \"corto\" en estos casos: no son suficientemente rápidos como para realizar todos los cálculos en un tiempo razonable, y la memoria es muy pequeña.\n",
    "\n",
    "La computación de alto desempeño, o High Performance Computing (HPC), es la disciplina que se hace cargo de este problema. La principal herramienta que se usa en HPC es la computación paralela, donde se usan varios procesadores coordinados haciendo cálculos al mismo tiempo. Otra opción, que se puede usar junto a la programación paralela, es el uso de aceleradores. Este notebook trata del uso de tarjetas gráficas (Graphic Processing Units o GPU) como aceleradores, desde Python.\n",
    "\n",
    "Como su nombre lo dice, historicamente las tarjetas gráficas han sido usadas para hacer la gráfica en la pantalla, sin embargo, desde mediados de los años 2000, comenzaron desarrollos que utilizaron las GPUs para otros propósitos, y nació el concepto General Purpose GPU.\n",
    "\n",
    "### Python y HPC\n",
    "\n",
    "A pesar que Python es un lenguaje que prioriza facilidad en uso por sobre eficiencia, existen herramientas que permiten hacer computación de alto desempeño con Python. Por ejemplo, la librería [`mpi4py`](http://mpi4py.scipy.org/) contiene funciones que permiten operar sobre más de un procesador y coordinarlos. También, la librería [`petsc4py`](https://pypi.python.org/pypi/petsc4py) da acceso a `PETSc`, una librería de computación paralela desarrollada en `C++` para aplicaciones numéricas.\n",
    "\n",
    "Por parte de aceleradores, existen librerías que permiten acceder a ellos desde Python. [`PyCUDA`](https://mathema.tician.de/software/pycuda) y [`PyOpenCL`](https://mathema.tician.de/software/pyopencl), desarrollados por [Andreas Kloeckner](https://mathema.tician.de/) de University of Illinois Urbana-Champaign, son dos ejemplos de esto. También, la versión pagada de la distribución de Python de [Numba](http://numba.pydata.org/) puede usar la GPU. En este notebook vamos a revisar las bases de `PyCUDA`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Programación en tarjetas gráficas\n",
    "\n",
    "### La tarjeta gráfica\n",
    "\n",
    "La tarjeta gráfica es un procesador con muchos núcleos. Nosotros estamos acostumbrados a hablar de un procesador (o CPU) de 2 o 4 núcleos, la GPU tiene miles. Eso si, los núcleos de la GPU son menos \"inteligentes\" que los de un procesador común: la GPU es un procesador de gran capacidad para hacer muchos cómputos simples de forma paralela. Si el problema puede ser descompuesto en muchos cálculos simples, la GPU es una muy buena alternativa, en cambio, si el código es complicado (por ejemplo, contiene muchos `if`), es probable que un procesador común tenga mejor desempeño.\n",
    "\n",
    "**PONER FIGURA GPU vs CPU**\n",
    "\n",
    "### CUDA (Computer Unified Device Architecture)\n",
    "\n",
    "Existen dos lenguajes para programar tarjetas gráficas: `CUDA` y `OpenCL`. Ambos funcionan de manera muy parecida, y en este caso nos focalizaremos en `CUDA`. `CUDA` es muy parecido al lenguaje `C`, con palabras clave cuando queremos que el código corra en la GPU. \n",
    "\n",
    "`CUDA` reconoce dos dominios o procesadores: `host` corresponde a la CPU y `device` a la GPU. Por defecto, el código corre en el `host`, sin embargo, hay palabras claves que le indica al computador que una función corra en el `device`. Esta función de conoce como `kernel`, y se reconocen porque hay que poner la palabra clave `__global__` antes de la definición de la función, por ejemplo:\n",
    "\n",
    "```C\n",
    "__global__ int suma_1(in a)\n",
    "{\n",
    "    int b = a + 1\n",
    "    return b\n",
    "}\n",
    "```\n",
    "\n",
    "El ``kernel`` es ejecutado en paralelo por cada núcleo de la GPU. Para una mejor organización, `CUDA` virtualiza los núcleos en hebras (o `threads`), que están organizados en bloques (o `blocks`). \n",
    "\n",
    "**IMAGEN THREADS Y BLOCKS**\n",
    "\n",
    "Dentro del `kernel` tenemos acceso al número del `thread` y `block` que está ejecutando el kernel con `threadIdx.x` y `blockIdx.x`, respectivamente. De esta forma, podemos hacer que ls `threads` operen sobre diferentes partes de la data. Por ejemplo, si queremos sumarle `1` a un arreglo de tamaño `N`, en vez de hacer un loop `for` que corra por todo el arreglo y sume `1`, podemos ejecutar un `kernel` en la GPU donde el `thread` 0 opere sobre el primer elemento del arreglo, el `tread` 1 sobre el segundo, y así sucesivamente. En `C/C++` una función que sume `1` a cada elemento de un arreglo `A` y guarda el resultado en el arreglo `B` sería\n",
    "\n",
    "```C\n",
    "void suma_1(float *A, float *B, int N)\n",
    "{\n",
    "    for (int i=0; i<N; i++)\n",
    "        B[i] = A[i] + 1; \n",
    "}\n",
    "```\n",
    "\n",
    "en cambio, de correr en la GPU no necesitamos el loop y quedaría\n",
    "\n",
    "```C\n",
    "__global__ void suma_1(float *A, float *C, int N)\n",
    "{\n",
    "    int i = threadIdx.x + blockDim.x*blockIdx.x;\n",
    "    \n",
    "    if (i<N)\n",
    "        B[i] = A[i] + 1; \n",
    "}\n",
    "```\n",
    "\n",
    "El cambio parece muy fácil, sin embargo hay un pequeño detalle: la memoria de la GPU y la CPU no es la misma. Por lo tanto, antes de correr el `kernel`, debemos mover la data que necesitaremos a la memoria de la GPU (en el ejemplo, `A` y `B`):\n",
    "\n",
    "```C\n",
    "float *A, *B;\n",
    "cudaMalloc( (void**) &A, N*sizeof(float));\n",
    "cudaMalloc( (void**) &B, N*sizeof(float));\n",
    "cudaMemcpy(A, A_h, N*sizeof(float), cudaMemcpyHostToDevice);\n",
    "cudaMemcpy(B, B_h, N*sizeof(float), cudaMemcpyHostToDevice);\n",
    "```\n",
    "donde `A_h` y `B_h` son los arreglos que están en la memoria de la CPU.\n",
    "\n",
    "y una vez finalizada la ejecución del `kernel`, debemos traer el resultado a la CPU:\n",
    "\n",
    "```C\n",
    "cudaMemcpy(B, B_d, N*sizeof(float), cudaMemcpyDeviceToHost);\n",
    "```\n",
    "\n",
    "La llamada al `kernel` también tiene una pequeña diferencia con `C`:\n",
    "\n",
    "```C\n",
    "int blocks = int N/256;\n",
    "suma_1<<<blocks, 256>>> (A, B, N);\n",
    "```\n",
    "donde `N` es el número de datos en el arreglo `A`, `256` es el número de `threads` que contiene cada `block` y `blocks` es el número de bloques necesarios para poder operar sobre toda la data. Pareciera ser que elegimos `256` de forma aleatoria, y de cierta manera, es verdad, sin embargo, generalmente funciona bien pues es un múltiplo del número de núcleos en la GPU. \n",
    "\n",
    "¿Más detalles? Vean la [guía de programación en CUDA](http://docs.nvidia.com/cuda/cuda-c-programming-guide/#axzz47plA3N1c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accediendo a la tarjeta gráfica desde Python: PyCUDA\n",
    "\n",
    "Ya vimos que `CUDA` nos permite acceder a la GPU desde `C`; `PyCUDA` nos permite hacerlo desde Python. Antes de cualquier cosa, nos tenemos que hacer dos preguntas:\n",
    "\n",
    "* ¿Mi GPU tiene soporte para `CUDA`?\n",
    "    Hay una [lista](https://developer.nvidia.com/cuda-gpus) de los modelos de GPU que soportan `CUDA`. Ve si la tarjeta gráfica en tu computador está en esa lista!\n",
    "* ¿Cómo instalo PyCUDA? \n",
    "    Revisen las instrucciones en [página de `PyCUDA`](https://wiki.tiker.net/PyCuda/Installation).\n",
    "\n",
    "Existen dos formas de acceder a la GPU usando `PyCUDA`: usando funciones prehechas que usan la GPU, o generando sus propios `kernels`. Veamos estas dos por separado.\n",
    "\n",
    "### Funciones de PyCUDA\n",
    "\n",
    "`PyCUDA` viene con una interfaz tipo `numpy`, cuyas funciones corren en la GPU. Esta es la forma más fácil de usar la GPU, ya que no es necesario hacer un `kernel`. Existe una [lista](https://documen.tician.de/pycuda/array.html#vector-types) con las funciones disponibles en `PyCUDA` usando esta interfaz.\n",
    "\n",
    "Para decirle a Python que vamos a usar la GPU, debemos importar `autoinit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pycuda.autoinit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y las principales funciones de `PyCUDA` están en el `driver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, así como `numpy`, `PyCUDA` tiene su propia estructura de datos. Por ejemplo, lo equivalente a un arreglo de `numpy` es un `gpuarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pycuda.gpuarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando un arreglo es un `gpuarray`, se puede mover la data a la GPU con la función `to_gpu`. Por ejemplo, si creamos un arreglo de `numpy` lleno de `1` y queremos mover esa data a la GPU hacemos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "a_cpu = numpy.ones(10)\n",
    "a_gpu = gpuarray.to_gpu(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos operar sobre `a_gpu` como si fuera un arreglo de `numpy`. Por ejemplo, para sumar 1 a su valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a2_gpu = a_gpu+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y luego podemos traerlo a la CPU con `get()` para imprimir su valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a2_cpu = a2_gpu.get()\n",
    "print a2_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generando nuestros propios kernels\n",
    "\n",
    "La cantidad de funciones disponibles en `PyCUDA` es limitada, y es muy probable que, de necesitar algo específico, sea necesario hacer un `kernel` especial. En `PyCUDA` el `kernel` es exactamente igual que en `CUDA`, y se escribe dentro de un `string`. `PyCUDA` usa el compilador de `CUDA`, llamado `nvcc`, para compilar lo que está dentro del `string` cuando se ejecuta el código Python. `PyCUDA` guarda el código compilado, así es que solamente recompila el `string` cuando ve que hay una modificación.\n",
    "\n",
    "Usemos el mismo ejemplo que antes, y sumemos 1 a los valores de un arreglo, esta vez, haciendo el `kernel` a mano. La función que compila el string con el kernel se llama `SourceModule`, y hacemos a esa función accesible por Python con `get_function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "kernel = SourceModule(\"\"\"\n",
    "__global__ void suma_1_gpu(float *A, float *B, int N)\n",
    "{\n",
    "    int i = threadIdx.x + blockDim.x*blockIdx.x;\n",
    "    if i<N\n",
    "        B[i] = A[i] + 1;\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "suma_1 = kernel.get_function(\"suma_1_gpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y podemos ejecutar el `kernel` de forma similar a `CUDA`, especificando el número de `threads` y `blocks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
